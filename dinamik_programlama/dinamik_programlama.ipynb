{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dinamik_programlama.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUWDytjtDvpJ"
      },
      "source": [
        
        "\n",
        "import random\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "class GridworldEnv():\n",
        "  def __init__(self):\n",
        "    self.shape = [4,4]\n",
        "    self.state_space = 16\n",
        "    self.action_space = 4\n",
        "\n",
        "\n",
        "  def next_state_probabilities(self):\n",
        "    next_state_probs = []\n",
        "    for state_number in range(self.state_space):\n",
        "      state_prob = []\n",
        "      if(state_number == 0 or state_number == 15):\n",
        "        done = True\n",
        "        reward = 0\n",
        "        for action_pos in range(self.action_space):\n",
        "          state_prob.append((1,state_number, reward, done))\n",
        "        next_state_probs.append(state_prob)\n",
        "      else:\n",
        "        for action_pos in range(self.action_space):\n",
        "          done = False\n",
        "          reward = -1\n",
        "          if(action_pos == 3):\n",
        "            if(state_number % 4 == 0):\n",
        "              ns = state_number\n",
        "            else:\n",
        "              ns = state_number -1 \n",
        "          if(action_pos == 1):\n",
        "            if(state_number%4 == 3):\n",
        "              ns = state_number\n",
        "            else:\n",
        "              ns = state_number +1\n",
        "          if(action_pos == 0 ):\n",
        "            if(int(state_number / 4) == 0):\n",
        "              ns = state_number\n",
        "            else :\n",
        "              ns = state_number - 4\n",
        "          if (action_pos == 2):\n",
        "            if(int(state_number/4)==3):\n",
        "              ns = state_number\n",
        "            else:\n",
        "              ns = state_number +4\n",
        "          state_prob.append((1,ns,reward,done))\n",
        "        next_state_probs.append(state_prob)\n",
        "    return next_state_probs"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-QkSu8SuOkd"
      },
      "source": [
        "def policy_evaluation(policy, env, discount_factor = 1 , theta = 0.0001):\n",
        "  V = np.zeros(env.state_space)\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(env.state_space):\n",
        "      v = 0\n",
        "      for a, action_prob in enumerate(policy[s]):\n",
        "        prob, next_state, reward, done = env.next_state_probabilities()[s][a]\n",
        "        v += action_prob * prob * (reward + discount_factor*V[next_state])\n",
        "      delta = max(delta, np.abs(v - V[s]))\n",
        "      V[s]= v\n",
        "    if(delta < theta):\n",
        "      break\n",
        "  return V\n",
        "\n",
        "#env = GridworldEnv()\n",
        "#policy = np.ones([env.state_space, env.action_space])/env.action_space\n",
        "#print(policy_evaluation(policy,env))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXDgPNBJ36s9"
      },
      "source": [
        "def policy_improvement(env,discount_factor=1):\n",
        "  policy = np.ones([env.state_space,env.action_space]) / env.action_space\n",
        "  while True:\n",
        "    V = policy_evaluation(policy, env)\n",
        "    policy_stable = True\n",
        "    for s in range(env.state_space):\n",
        "      action_values = np.zeros(env.action_space)\n",
        "      for a in range(env.action_space):\n",
        "        prob, next_state , reward , done = env.next_state_probabilities()[s][a]\n",
        "        action_values[a] += prob * (reward + discount_factor*V[next_state])\n",
        "      choosen_a = np.argmax(policy[s])\n",
        "      best_action = np.argmax(action_values)\n",
        "      if(choosen_a != best_action):\n",
        "        policy_stable = False\n",
        "      policy[s] = np.eye(env.action_space)[best_action]\n",
        "    if(policy_stable == True):\n",
        "      return policy, V"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gHMK43B2FK",
        "outputId": "fa123f99-6599-4320-aed6-c5382dd839a5"
      },
      "source": [
        "def value_iteration(env,theta=0.001, discount_factor=1):\n",
        "  V = np.zeros(env.state_space)\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(env.state_space):\n",
        "      A = np.zeros(env.action_space)\n",
        "      for a in range(env.action_space):\n",
        "        prob, next_state, reward , done = env.next_state_probabilities()[s][a]\n",
        "        A[a] += prob * (reward + discount_factor*V[next_state])\n",
        "      best_action = max(A)\n",
        "      delta = max(delta, np.abs(best_action - V[s]))\n",
        "      V[s] = best_action\n",
        "    if(delta< theta):\n",
        "      break\n",
        "  \n",
        "  policy = np.zeros([env.state_space,env.action_space])\n",
        "  for s in range(env.state_space):\n",
        "    A = np.zeros(env.action_space)\n",
        "    for a in range(env.action_space):\n",
        "      prob, next_state, reward , done = env.next_state_probabilities()[s][a]\n",
        "      A[a] += prob * (reward + discount_factor*V[next_state])\n",
        "    best_a = np.argmax(A)\n",
        "    policy[s][best_a] = 1\n",
        "  return policy, V\n",
        "env = GridworldEnv()\n",
        "print(value_iteration(env))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[1., 0., 0., 0.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [1., 0., 0., 0.]]), array([ 0., -1., -2., -3., -1., -2., -3., -2., -2., -3., -2., -1., -3.,\n",
            "       -2., -1.,  0.]))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
